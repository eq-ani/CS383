{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Equation of a Slime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many late days are you using for this assignment? 0 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Import functions from Scikit-Learn for regression, classification, and cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 rows of the dataset:\n",
      "    Temperature °C  Mols KCL     Size nm^3\n",
      "0              469       647  6.244743e+05\n",
      "1              403       694  5.779610e+05\n",
      "2              302       975  6.196847e+05\n",
      "3              779       916  1.460449e+06\n",
      "4              901        18  4.325726e+04\n",
      "5              545       637  7.124634e+05\n",
      "6              660       519  7.006960e+05\n",
      "7              143       869  2.718260e+05\n",
      "8               89       461  8.919803e+04\n",
      "9              294       776  4.770210e+05\n",
      "10             991       117  2.441771e+05\n",
      "11             307       781  5.006455e+05\n",
      "12             206        70  3.145200e+04\n",
      "13             437       599  5.390215e+05\n",
      "14             566        75  9.185271e+04\n",
      "\n",
      "Dataset Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Temperature °C  1000 non-null   int64  \n",
      " 1   Mols KCL        1000 non-null   int64  \n",
      " 2   Size nm^3       1000 non-null   float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using pandas load the dataset\n",
    "data_slime = pd.read_csv('science_data_large.csv')\n",
    "# Output the first 15 rows of the data\n",
    "print(\"First 15 rows of the dataset:\")\n",
    "print(data_slime.head(15))\n",
    "# Display a summary of the table information (data types, non-null counts, etc.)\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(data_slime.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the pandas dataset and split it into our features (X) and label (y)\n",
    "X = data_slime.iloc[:, :-1]\n",
    "y = data_slime.iloc[:, -1]\n",
    "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
    "# For grading consistency use random_state=42 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datapoint (features):      Temperature °C  Mols KCL\n",
      "521             100       541\n",
      "Prediction for the sample datapoint: [235911.1927226]\n",
      "Model Score (R^2): 0.8552472077276095\n",
      "Coefficients: [ 866.14641337 1032.69506649]\n",
      "Intercept: -409391.4795834075\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to train a model on the training set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Create a sample datapoint and predict the output of that sample with the trained model\n",
    "sample_datapoint_df = X_test.iloc[[0]]\n",
    "sample_prediction = model.predict(sample_datapoint_df)\n",
    "print(\"Sample datapoint (features):\", sample_datapoint_df)\n",
    "print(\"Prediction for the sample datapoint:\", sample_prediction)\n",
    "# Report the score for that model using the default score function property of the SKLearn model, \n",
    "# in your own words (markdown, not code) explain what the score means\n",
    "print(\"Model Score (R^2):\", model.score(X_test, y_test))\n",
    "# Extract the coefficients and intercept from the model and write an equation for your h(x) using LaTeX\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the linear equation of a slime: $h(x)=−409391.47958 + 866.14641⋅x_1 + 1032.69507⋅x_2$\n",
    "\n",
    "FOR REFERENCE: x1 represents temperature (C) and x2 represents mol (KCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on score and explain meaning: The R² score of 0.855272 means that \n",
    "approximately 85.53% of the variance in the target variable \n",
    "(the change in slime size) is explained by the independent \n",
    "variables (KCl concentration and temperature) in the model. In other \n",
    "words, the model's predictions capture 85.53% of the variability in the\n",
    "data, while the remaining 14.47% of the variability is due to other\n",
    "factors or noise not captured by this linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (R² for each fold): [0.86151889 0.82742341 0.87195173 0.88166206 0.85609101]\n",
      "Mean CV Score (R²): 0.8597294202684646\n",
      "Standard Deviation: 0.01838773713930639\n"
     ]
    }
   ],
   "source": [
    "# Use the cross_val_score function to repeat your experiment across many shuffles of the data\n",
    "# For grading consistency use n_splits=5 and random_state=42\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf)\n",
    "print(\"Cross-validation scores (R² for each fold):\", cv_scores)\n",
    "print(\"Mean CV Score (R²):\", cv_scores.mean())\n",
    "print(\"Standard Deviation:\", cv_scores.std())\n",
    "# Report on their finding and their significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write findings here: The mean score is 0.85973, which is very high for datasets such as these. This score indicates that, on average, the model explains about 85.97% of the variance in the target variable—a strong performance given the nature of the data. Additionally, the standard deviation is very low (0.01839), meaning that the model's performance is consistent across each of the data subsets used in the cross-validation process. This low variability highlights the robustness of the model and suggests that it generalizes well to unseen data. Overall, these results provide strong evidence that the model is both accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (R² for each fold, polynomial regression): [1. 1. 1. 1. 1.]\n",
      "Mean CV Score (R², polynomial regression): 1.0\n",
      "Standard Deviation (polynomial regression): 0.0\n",
      "Coefficients (polynomial): [ 0.00000000e+00  1.20000000e+01 -1.23111325e-07 -1.05668034e-11\n",
      "  2.00000000e+00  2.85714287e-02]\n",
      "Intercept (polynomial): 1.6572012100368738e-05\n"
     ]
    }
   ],
   "source": [
    "# Using the PolynomialFeatures library perform another regression on an augmented dataset of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model_poly = LinearRegression()\n",
    "# Perform k-fold cross validation (as above)\n",
    "kf_poly = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "cv_scores_poly = cross_val_score(model_poly, X_poly, y, cv=kf_poly)\n",
    "\n",
    "print(\"Cross-validation scores (R² for each fold, polynomial regression):\", cv_scores_poly)\n",
    "print(\"Mean CV Score (R², polynomial regression):\", cv_scores_poly.mean())\n",
    "print(\"Standard Deviation (polynomial regression):\", cv_scores_poly.std())\n",
    "\n",
    "model_poly.fit(X_poly, y)\n",
    "coefficients_poly = model_poly.coef_\n",
    "intercept_poly = model_poly.intercept_\n",
    "print(\"Coefficients (polynomial):\", coefficients_poly)\n",
    "print(\"Intercept (polynomial):\", intercept_poly)\n",
    "# Report on the metrics and output the resultant equation as you did in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the polynomial equation of a slime: $h(x) = 1.6572×10^{-5} + 12x_1 - 1.2311×10^{-7}x_2 - 1.05668×10^{-11}x_1^2 + 2.0x_1x_2 + 2.8571×10^{-2}x_2^2$ \n",
    "\n",
    "FOR REFERENCE: x1 represents temperature (C) and x2 represents mol (KCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on the score and interpret: The R² score of 1.0 means that approximately 100% of the variance in the target variable (Size nm³) is explained by the independent variables (Temperature °C and Mols KCL) and their quadratic interactions in the model. In other words, the model's predictions capture 100% of the variability in the data, leaving no unexplained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Chronic Kidney Disease Prediction via Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create code and markdown cells as needed to perform classification and report on your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Classification Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Results (5-Fold Cross-Validation):\n",
      "                      Model  Mean Accuracy  Std Deviation\n",
      "0       Logistic Regression       0.856559       0.066269\n",
      "1    Support Vector Machine       0.928172       0.047601\n",
      "2       k-Nearest Neighbors       0.927957       0.052440\n",
      "3  Neural Network (Default)       0.935054       0.040466\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset. Then train and evaluate the classification models.\n",
    "df_ckd = pd.read_csv('ckd_feature_subset.csv')\n",
    "X = df_ckd.drop('Target_ckd', axis=1)\n",
    "y = df_ckd['Target_ckd']\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Neural Network (Default)\": MLPClassifier(max_iter=1000, random_state=42)\n",
    "}\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=kf)\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    std_score = np.std(cv_scores)\n",
    "    results.append([model_name, mean_score, std_score])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Mean Accuracy\", \"Std Deviation\"])\n",
    "print(\"Classification Results (5-Fold Cross-Validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Conclusion for Classification Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Among the classification models, logistic regression achieved around 85.66% accuracy. Both the support vector machine and k-nearest neighbors models performed similarly well, each reaching about 92.82% accuracy. The default neural network further improved the performance to approximately 93.51% accuracy with the lowest variability. These results underscore the advantage of non-linear models for this CKD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Neural Network Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Configuration Results (5-Fold Cross-Validation):\n",
      "                       Configuration  Mean Accuracy  Std Deviation\n",
      "0          NN Config 1: (10,) | relu       0.921720       0.056307\n",
      "1       NN Config 2: (50, 25) | tanh       0.954624       0.032815\n",
      "2  NN Config 3: (100, 50, 25) | relu       0.967527       0.035340\n"
     ]
    }
   ],
   "source": [
    "# Experiments with Neural Network.\n",
    "nn_configurations = [\n",
    "    {\"hidden_layer_sizes\": (10,), \"activation\": \"relu\"},\n",
    "    {\"hidden_layer_sizes\": (50, 25), \"activation\": \"tanh\"},\n",
    "    {\"hidden_layer_sizes\": (100, 50, 25), \"activation\": \"relu\"}\n",
    "]\n",
    "\n",
    "nn_results = []\n",
    "\n",
    "for i, config in enumerate(nn_configurations, start=1):\n",
    "    nn_model = MLPClassifier(\n",
    "        hidden_layer_sizes=config[\"hidden_layer_sizes\"],\n",
    "        activation=config[\"activation\"],\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "    cv_scores = cross_val_score(nn_model, X, y, cv=kf)\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    std_score = np.std(cv_scores)\n",
    "    label = f\"NN Config {i}: {config['hidden_layer_sizes']} | {config['activation']}\"\n",
    "    nn_results.append([label, mean_score, std_score])\n",
    "nn_results_df = pd.DataFrame(nn_results, columns=[\"Configuration\", \"Mean Accuracy\", \"Std Deviation\"])\n",
    "print(\"Neural Network Configuration Results (5-Fold Cross-Validation):\")\n",
    "print(nn_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Conclusion for Neural Network Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Increasing the model complexity improved performance on the CKD dataset. The simplest configuration (NN Config 1 with one hidden layer of 10 neurons and ReLU activation) achieved about 90.86% accuracy (±5.20%). NN Config 2 (two hidden layers of 50 and 25 neurons with tanh) improved accuracy to roughly 95.46% (±3.28%), while the most complex configuration (NN Config 3 with three hidden layers of 100, 50, and 25 neurons using ReLU) reached approximately 96.75% accuracy (±3.53%). This indicates that deeper networks capture the underlying patterns more effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
